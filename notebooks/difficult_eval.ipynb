{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difficult Eval\n",
    "I evaluated the stuff in fine_tuning with different datasets. This is not good. I will re-evaluate according to one test dataset and MATCH the order of the errors (thankfully I used the same seed to shuffle the data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import pyarrow as pa\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset, DatasetDict\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from utils import *\n",
    "from sft import *\n",
    "from constants import SYSTEM_INSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_CONVERSIONS = {\n",
    "                    \"correct\": '0',\n",
    "                    \"intrinsic-NP\": '1',\n",
    "                    \"intrinsic-predicate\": '2',\n",
    "                    \"extrinsic-NP\": '3',\n",
    "                    \"extrinsic-predicate\": '4'}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B-Instruct')\n",
    "\n",
    "def formatting_prompts_func(example, training=True):\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"error_type\"])):\n",
    "        text = f\"{SYSTEM_INSTRUCTION}\\n ### Text1: {example['doc'][i]}\\n ### Text2: {example['summ'][i]}\\n ### Output: \"\n",
    "        if training:\n",
    "            text += (\n",
    "                f\"{LABEL_CONVERSIONS[example['error_type'][i]]} .\" + tokenizer.eos_token\n",
    "            )\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "def reformat_data_split_labels(dataset, dataset_name='Lislaam/AggreFact'):\n",
    "    \"\"\"Reformats the dataset to have the same format for all datasets for consistency.\n",
    "    Args:\n",
    "        dataset: dataset -- dataset to reformat\n",
    "        dataset_name: str -- name of the dataset\n",
    "    Returns:\n",
    "        dataset: dataset -- reformatted dataset\n",
    "    \"\"\"\n",
    "    def duplicate_and_label(example):\n",
    "        \"\"\"Duplicates examples with multiple error types, assigning one label per duplicate.\"\"\"\n",
    "        ids = []\n",
    "        docs = []\n",
    "        summs = []\n",
    "        labels = []\n",
    "        \n",
    "        if example['errors'] is not None:\n",
    "            try:\n",
    "                lst = ast.literal_eval(example['errors'])\n",
    "                for label in lst:\n",
    "                    ids.append(example['id'])\n",
    "                    docs.append(example['doc'])\n",
    "                    summs.append(example['summ'])\n",
    "                    labels.append(label)\n",
    "            except ValueError:  # If 'errors' is not a list, e.g., it is 'correct'\n",
    "                ids.append(example['id'])\n",
    "                docs.append(example['doc'])\n",
    "                summs.append(example['summ'])\n",
    "                labels.append(example['errors'])\n",
    "        return [{'id': id, 'doc': doc, 'summ': summ, 'error_type': label} for id, doc, summ, label in zip(ids, docs, summs, labels)]\n",
    "    def process_in_chunks(dataset, chunk_size=10000, map_function=duplicate_and_label):\n",
    "        chunked_tables = dataset.data.to_batches(max_chunksize=chunk_size)\n",
    "        processed_chunks = []\n",
    "        \n",
    "        for chunk in chunked_tables:\n",
    "            # Convert chunk to a PyArrow table\n",
    "            chunk_table = pa.Table.from_batches([chunk])\n",
    "            \n",
    "            # Convert the chunk table to a pandas DataFrame\n",
    "            chunk_df = chunk_table.to_pandas()\n",
    "            \n",
    "            if map_function:\n",
    "                # Rename the column before splitting lists of errors into separate examples\n",
    "                chunk_df = chunk_df.rename(columns={'error_type': 'errors'})\n",
    "                \n",
    "                # Apply the map function and flatten the result\n",
    "                flattened_rows = chunk_df.apply(lambda row: map_function(row.to_dict()), axis=1).sum()\n",
    "                \n",
    "                # Convert the flattened list of dictionaries to a DataFrame\n",
    "                chunk_df = pd.DataFrame(flattened_rows)\n",
    "            \n",
    "            processed_chunks.append(chunk_df)\n",
    "        \n",
    "        # Combine all processed chunks back into a single DataFrame\n",
    "        combined_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "        \n",
    "        return Dataset.from_pandas(combined_df)\n",
    "    if dataset_name == \"Lislaam/AggreFact\":\n",
    "        error_types = ['correct', 'intrinsic-NP', 'intrinsic-predicate', 'extrinsic-NP', 'extrinsic-predicate']\n",
    "        dataset = process_in_chunks(dataset)\n",
    "        dataset = dataset.filter(lambda x: x['error_type'] in error_types)\n",
    "        #dataset = dataset.filter(lambda x: len(x['doc']) < 1800)\n",
    "        #dataset = dataset.map(error_type_map)\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {dataset_name} not supported.\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def oversampling(dataset, error_types=['correct', 'intrinsic-NP', 'intrinsic-predicate', 'extrinsic-NP', 'extrinsic-predicate'], n=2330):\n",
    "    def replicate_class(dataset, error_type, n):\n",
    "        filtered = dataset.filter(lambda x: x['error_type'] == error_type)\n",
    "        num_examples = len(filtered)\n",
    "        \n",
    "        if num_examples == 0:\n",
    "            return filtered  # Return empty dataset if no examples\n",
    "        \n",
    "        # Calculate how many times to replicate the dataset\n",
    "        num_repeats = n // num_examples\n",
    "        num_remaining = n % num_examples\n",
    "        \n",
    "        # Repeat the dataset and select the needed number of examples\n",
    "        replicated = concatenate_datasets([filtered] * num_repeats)\n",
    "        remaining = filtered.shuffle(seed=42).select(range(num_remaining))\n",
    "        \n",
    "        # Concatenate the replicated examples with the additional ones needed\n",
    "        return concatenate_datasets([replicated, remaining])\n",
    "    # Initialize an empty dataset for oversampling\n",
    "    oversampled_dataset = Dataset.from_dict({\n",
    "        'doc': [],\n",
    "        'summ': [],\n",
    "        'error_type': []\n",
    "    })\n",
    "    for error_type in error_types:\n",
    "        oversampled = replicate_class(dataset, error_type, n)\n",
    "        oversampled_dataset = concatenate_datasets([oversampled_dataset, oversampled])\n",
    "    # Shuffle the final dataset\n",
    "    oversampled_dataset = oversampled_dataset.shuffle(seed=42)\n",
    "    return oversampled_dataset\n",
    "\n",
    "\n",
    "def undersampling(dataset, error_types=['correct', 'intrinsic-NP', 'intrinsic-predicate', 'extrinsic-NP', 'extrinsic-predicate'],\n",
    "                    n=400):\n",
    "    def sample_class(dataset, error_type, n):\n",
    "        filtered = dataset.filter(lambda x: x['error_type'] == error_type)\n",
    "        return filtered.shuffle(seed=42).select(range(min(n, len(filtered))))\n",
    "\n",
    "    # Sample 400 examples from each class\n",
    "    sampled_dataset = Dataset.from_dict({\n",
    "        'doc': [],\n",
    "        'summ': [],\n",
    "        'error_type': []\n",
    "    })\n",
    "\n",
    "    for error_type in error_types:\n",
    "        sampled = sample_class(dataset, error_type, n)\n",
    "        sampled_dataset = concatenate_datasets([sampled_dataset, sampled])\n",
    "\n",
    "    # Shuffle the final dataset\n",
    "    sampled_dataset = sampled_dataset.shuffle(seed=42)\n",
    "\n",
    "    return sampled_dataset\n",
    "\n",
    "\n",
    "def extract(json_file_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract predictions and labels into lists\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for entry in data:\n",
    "        true_labels.append(entry['label'])\n",
    "        predicted_labels.append(entry['prediction'])\n",
    "\n",
    "    return true_labels, predicted_labels\n",
    "\n",
    "\n",
    "def get_score(predictions, references):\n",
    "    #processed_preds = [preprocess(pred, model) for pred in predictions]\n",
    "    processed_refs = [preprocess(ref) for ref in references] # Should always be processable\n",
    "    flatten = lambda lst: [item for sublist in lst for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "    total = 0\n",
    "    class_errors = {'extrinsic-NP': 0, 'extrinsic-predicate': 0, 'intrinsic-NP': 0,\n",
    "                    'intrinsic-predicate': 0, 'correct': 0}\n",
    "    num_extrinsicnp = sum([1 for ref in flatten(processed_refs) if ref == 'extrinsic-NP']) if 'extrinsic-NP' in flatten(processed_refs) else 1\n",
    "    num_extrinsicpredicate = sum([1 for ref in flatten(processed_refs) if ref == 'extrinsic-predicate']) if 'extrinsic-predicate' in flatten(processed_refs) else 1\n",
    "    num_intrinsicnp = sum([1 for ref in flatten(processed_refs) if ref == 'intrinsic-NP']) if 'intrinsic-NP' in flatten(processed_refs) else 1\n",
    "    num_intrinsicpredicate = sum([1 for ref in flatten(processed_refs) if ref == 'intrinsic-predicate']) if 'intrinsic-predicate' in flatten(processed_refs) else 1\n",
    "    num_correct = sum([1 for ref in flatten(processed_refs) if ref == 'correct']) if 'correct' in flatten(processed_refs) else 1\n",
    "    # Check if any ref is within pred\n",
    "    for i in range(len(processed_refs)):\n",
    "        if type(processed_refs[i])==list:\n",
    "            for x in processed_refs[i]:\n",
    "                # print(processed_refs[i], x, predictions[i], soft_match(predictions[i], x), '/n')\n",
    "                if soft_match(predictions[i], x): # Check if that ref is in the pred\n",
    "                    total += 1/len(processed_refs[i])\n",
    "                    class_errors[x] += 1\n",
    "        else:\n",
    "            # print(processed_refs[i], predictions[i], soft_match(predictions[i], processed_refs[i]), '/n')\n",
    "            if soft_match(predictions[i], processed_refs[i]):\n",
    "                total += 1\n",
    "                class_errors[processed_refs[i]] += 1\n",
    "    scores = {'total': total / len(processed_refs),\n",
    "              'extrinsic-NP': class_errors[\"extrinsic-NP\"] / num_extrinsicnp if 'extrinsic-NP' in flatten(processed_refs) else None,\n",
    "              'extrinsic-predicate': class_errors[\"extrinsic-predicate\"] / num_extrinsicpredicate if 'extrinsic-predicate' in flatten(processed_refs) else None,\n",
    "              'intrinsic-NP': class_errors[\"intrinsic-NP\"] / num_intrinsicnp if 'intrinsic-NP' in flatten(processed_refs) else None,\n",
    "              'intrinsic-predicate': class_errors[\"intrinsic-predicate\"] / num_intrinsicpredicate if 'intrinsic-predicate' in flatten(processed_refs) else None,\n",
    "              'correct': class_errors[\"correct\"] / num_correct if 'correct' in flatten(processed_refs) else None}\n",
    "    \n",
    "    #print(processed_refs)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 6540/6540 [00:00<00:00, 199689.50 examples/s]\n",
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 97332.08 examples/s]\n",
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 95852.32 examples/s]\n",
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 95466.54 examples/s]\n",
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 94296.43 examples/s]\n",
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 95085.30 examples/s]\n",
      "Map: 100%|██████████| 6990/6990 [00:00<00:00, 36255.18 examples/s]\n",
      "Map: 100%|██████████| 2330/2330 [00:00<00:00, 40135.07 examples/s]\n",
      "Map: 100%|██████████| 2330/2330 [00:00<00:00, 35849.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"Lislaam/AggreFact\", split=['validation[:]', 'test[:]'])\n",
    "dataset = concatenate_datasets([dataset[0], dataset[1]]) # Turn into one dataset to make new split\n",
    "dataset = reformat_data_split_labels(dataset, \"Lislaam/AggreFact\") # Get rid of non-standard error_type examples and split data\n",
    "\n",
    "eval_dataset = oversampling(dataset)\n",
    "\n",
    "# Split the dataset into train and test sets (80% train, 20% test)\n",
    "train_test = eval_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Further split the train set into train and validation sets (75% train, 25% validation of the original 80%)\n",
    "train_valid = train_test['train'].train_test_split(test_size=0.25)\n",
    "\n",
    "# Combine the splits into a DatasetDict\n",
    "eval_dataset = DatasetDict({\n",
    "    'train': train_valid['train'],\n",
    "    'validation': train_valid['test'],\n",
    "    'test': train_test['test']\n",
    "})\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    lambda x: {\"formatted_text\": formatting_prompts_func(x, False)},\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'doc', 'summ', 'error_type', 'formatted_text'],\n",
       "        num_rows: 6990\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'doc', 'summ', 'error_type', 'formatted_text'],\n",
       "        num_rows: 2330\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'doc', 'summ', 'error_type', 'formatted_text'],\n",
       "        num_rows: 2330\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_index(test_set, eval_set=eval_dataset['test']):\n",
    "    # Step 1: Create an index for the large dataset\n",
    "    # Create a dictionary with 'summ' as the key and the index in the large dataset as the value\n",
    "    eval_set_list = eval_set['id']  # Extracting 'summ' column from eval_set\n",
    "    large_dataset_index = {summ: idx for idx, summ in enumerate(eval_set_list)}\n",
    "\n",
    "    # Step 2: Find the indices of the smaller dataset entries in the large dataset\n",
    "    test_set_list = test_set['id']  # Extracting 'summ' column from test_set\n",
    "    matching_indices = []\n",
    "    \n",
    "    for summ in test_set_list:\n",
    "        if summ in large_dataset_index:\n",
    "            matching_indices.append(large_dataset_index[summ])\n",
    "        else:\n",
    "            matching_indices.append(None)\n",
    "            #print(f\"Entry not found for: {summ}\")  # Handle case when entry is not found\n",
    "\n",
    "    return matching_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 94349.81 examples/s]\n",
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 94203.79 examples/s]\n",
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 94457.83 examples/s]\n",
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 96503.40 examples/s]\n",
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 95986.43 examples/s]\n",
      "Map: 100%|██████████| 6990/6990 [00:00<00:00, 42093.83 examples/s]\n",
      "Map: 100%|██████████| 2330/2330 [00:00<00:00, 38766.05 examples/s]\n",
      "Map: 100%|██████████| 2330/2330 [00:00<00:00, 40670.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data = oversampling(dataset)\n",
    "\n",
    "# Split the dataset into train and test sets (80% train, 20% test)\n",
    "train_test = data.train_test_split(test_size=0.2)\n",
    "\n",
    "# Further split the train set into train and validation sets (75% train, 25% validation of the original 80%)\n",
    "train_valid = train_test['train'].train_test_split(test_size=0.25)\n",
    "\n",
    "# Combine the splits into a DatasetDict\n",
    "data = DatasetDict({\n",
    "    'train': train_valid['train'],\n",
    "    'validation': train_valid['test'],\n",
    "    'test': train_test['test']\n",
    "})\n",
    "\n",
    "data = data.map(\n",
    "    lambda x: {\"formatted_text\": formatting_prompts_func(x, False)},\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for i in match_index(data['test']) if i==None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2330, 2330)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, preds = extract(\"fine_tuning safe copy/meta-llama/Meta-Llama-3-8B-Instruct/naive_oversampling/summary.json\")\n",
    "len(labels), len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2147, 2147)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_index = match_index(data['test'])\n",
    "trues = []\n",
    "labs = []\n",
    "\n",
    "for l in range(len(labels)):\n",
    "    if map_index[l] != None:\n",
    "        trues.append(eval_dataset['test'][l]['error_type'])\n",
    "        labs.append(labels[l])\n",
    "\n",
    "len(trues), len(labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 0.20353982300884957,\n",
       " 'extrinsic-NP': 0.19523809523809524,\n",
       " 'extrinsic-predicate': 0.1864801864801865,\n",
       " 'intrinsic-NP': 0.21123595505617979,\n",
       " 'intrinsic-predicate': 0.2116788321167883,\n",
       " 'correct': 0.21266968325791855}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score(labs, trues)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
