{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difficult Eval\n",
    "I evaluated the stuff in fine_tuning with different datasets. This is not good. I will re-evaluate according to one test dataset and MATCH the order of the errors (thankfully I used the same seed to shuffle the data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import pyarrow as pa\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset, DatasetDict\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary_dataset(dataset, error_type='correct'):\n",
    "    # Map dataset into error / not_error\n",
    "    def map_to_binary(x):\n",
    "        # Assume error_type is a list, convert to single string if needed\n",
    "        x['error_type'] = 'correct' if error_type in x['error_type'] else 'incorrect'\n",
    "        return x\n",
    "\n",
    "    # Apply the binary mapping\n",
    "    binary_dataset = dataset.map(map_to_binary)\n",
    "\n",
    "    # Count the number of 'correct' samples\n",
    "    num_correct = binary_dataset.filter(lambda x: x['error_type'] == 'correct').num_rows\n",
    "\n",
    "    # Undersample to ensure balanced classes\n",
    "    binary_dataset = undersampling(binary_dataset, error_types=['correct', 'incorrect'], n=num_correct)\n",
    "\n",
    "    return binary_dataset\n",
    "\n",
    "\n",
    "def undersampling(dataset, error_types=['correct', 'intrinsic-NP', 'intrinsic-predicate', 'extrinsic-NP', 'extrinsic-predicate'],\n",
    "                    n=400):\n",
    "    def sample_class(dataset, error_type, n):\n",
    "        filtered = dataset.filter(lambda x: x['error_type'] == error_type)\n",
    "        return filtered.shuffle(seed=42).select(range(min(n, len(filtered))))\n",
    "\n",
    "    # Sample 400 examples from each class\n",
    "    sampled_dataset = Dataset.from_dict({\n",
    "        'doc': [],\n",
    "        'summ': [],\n",
    "        'error_type': []\n",
    "    })\n",
    "\n",
    "    for error_type in error_types:\n",
    "        sampled = sample_class(dataset, error_type, n)\n",
    "        sampled_dataset = concatenate_datasets([sampled_dataset, sampled])\n",
    "\n",
    "    # Shuffle the final dataset\n",
    "    sampled_dataset = sampled_dataset.shuffle(seed=42)\n",
    "\n",
    "    return sampled_dataset\n",
    "\n",
    "\n",
    "def oversampling(dataset, error_types=['correct', 'intrinsic-NP', 'intrinsic-predicate', 'extrinsic-NP', 'extrinsic-predicate'], n=2330):\n",
    "    def replicate_class(dataset, error_type, n):\n",
    "        filtered = dataset.filter(lambda x: x['error_type'] == error_type)\n",
    "        num_examples = len(filtered)\n",
    "        \n",
    "        if num_examples == 0:\n",
    "            return filtered  # Return empty dataset if no examples\n",
    "        \n",
    "        # Calculate how many times to replicate the dataset\n",
    "        num_repeats = n // num_examples\n",
    "        num_remaining = n % num_examples\n",
    "        \n",
    "        # Repeat the dataset and select the needed number of examples\n",
    "        replicated = concatenate_datasets([filtered] * num_repeats)\n",
    "        remaining = filtered.shuffle(seed=42).select(range(num_remaining))\n",
    "        \n",
    "        # Concatenate the replicated examples with the additional ones needed\n",
    "        return concatenate_datasets([replicated, remaining])\n",
    "\n",
    "    # Initialize an empty dataset for oversampling\n",
    "    oversampled_dataset = Dataset.from_dict({\n",
    "        'doc': [],\n",
    "        'summ': [],\n",
    "        'error_type': []\n",
    "    })\n",
    "\n",
    "    for error_type in error_types:\n",
    "        oversampled = replicate_class(dataset, error_type, n)\n",
    "        oversampled_dataset = concatenate_datasets([oversampled_dataset, oversampled])\n",
    "\n",
    "    # Shuffle the final dataset\n",
    "    oversampled_dataset = oversampled_dataset.shuffle(seed=42)\n",
    "\n",
    "    return oversampled_dataset\n",
    "\n",
    "\n",
    "def reformat_data_split_labels(dataset, dataset_name):\n",
    "    \"\"\"Reformats the dataset to have the same format for all datasets for consistency.\n",
    "\n",
    "    Args:\n",
    "        dataset: dataset -- dataset to reformat\n",
    "        dataset_name: str -- name of the dataset\n",
    "\n",
    "    Returns:\n",
    "        dataset: dataset -- reformatted dataset\n",
    "    \"\"\"\n",
    "    def duplicate_and_label(example):\n",
    "        \"\"\"Duplicates examples with multiple error types, assigning one label per duplicate.\"\"\"\n",
    "        docs = []\n",
    "        summs = []\n",
    "        labels = []\n",
    "        \n",
    "        if example['errors'] is not None:\n",
    "            try:\n",
    "                lst = ast.literal_eval(example['errors'])\n",
    "                for label in lst:\n",
    "                    docs.append(example['doc'])\n",
    "                    summs.append(example['summ'])\n",
    "                    labels.append(label)\n",
    "            except ValueError:  # If 'errors' is not a list, e.g., it is 'correct'\n",
    "                docs.append(example['doc'])\n",
    "                summs.append(example['summ'])\n",
    "                labels.append(example['errors'])\n",
    "\n",
    "        return [{'doc': doc, 'summ': summ, 'error_type': label} for doc, summ, label in zip(docs, summs, labels)]\n",
    "\n",
    "    def process_in_chunks(dataset, chunk_size=10000, map_function=duplicate_and_label):\n",
    "        chunked_tables = dataset.data.to_batches(max_chunksize=chunk_size)\n",
    "        processed_chunks = []\n",
    "        \n",
    "        for chunk in chunked_tables:\n",
    "            # Convert chunk to a PyArrow table\n",
    "            chunk_table = pa.Table.from_batches([chunk])\n",
    "            \n",
    "            # Convert the chunk table to a pandas DataFrame\n",
    "            chunk_df = chunk_table.to_pandas()\n",
    "            \n",
    "            if map_function:\n",
    "                # Rename the column before splitting lists of errors into separate examples\n",
    "                chunk_df = chunk_df.rename(columns={'error_type': 'errors'})\n",
    "                \n",
    "                # Apply the map function and flatten the result\n",
    "                flattened_rows = chunk_df.apply(lambda row: map_function(row.to_dict()), axis=1).sum()\n",
    "                \n",
    "                # Convert the flattened list of dictionaries to a DataFrame\n",
    "                chunk_df = pd.DataFrame(flattened_rows)\n",
    "            \n",
    "            processed_chunks.append(chunk_df)\n",
    "        \n",
    "        # Combine all processed chunks back into a single DataFrame\n",
    "        combined_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "        \n",
    "        return Dataset.from_pandas(combined_df)\n",
    "\n",
    "    if dataset_name == \"Lislaam/AggreFact\":\n",
    "        error_types = ['correct', 'intrinsic-NP', 'intrinsic-predicate', 'extrinsic-NP', 'extrinsic-predicate']\n",
    "        dataset = process_in_chunks(dataset)\n",
    "        dataset = dataset.filter(lambda x: x['error_type'] in error_types)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {dataset_name} not supported.\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 6540/6540 [00:00<00:00, 237433.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "DATA = \"Lislaam/AggreFact\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(DATA, split=['validation[:]', 'test[:]'])\n",
    "dataset = concatenate_datasets([dataset[0], dataset[1]]) # Turn into one dataset to make new split\n",
    "dataset = reformat_data_split_labels(dataset, DATA) # Get rid of non-standard error_type examples and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 119505.67 examples/s]\n",
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 114659.63 examples/s]\n",
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 120324.40 examples/s]\n",
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 120075.40 examples/s]\n",
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 122049.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = oversampling(dataset)\n",
    "\n",
    "# Split the dataset into train and test sets (80% train, 20% test)\n",
    "train_test = eval_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Further split the train set into train and validation sets (75% train, 25% validation of the original 80%)\n",
    "train_valid = train_test['train'].train_test_split(test_size=0.25)\n",
    "\n",
    "# Combine the splits into a DatasetDict\n",
    "eval_dataset = DatasetDict({\n",
    "    'train': train_valid['train'],\n",
    "    'validation': train_valid['test'],\n",
    "    'test': train_test['test']\n",
    "})\n",
    "\n",
    "eval = eval_dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 121169.19 examples/s]\n",
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 123396.21 examples/s]\n",
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 124467.98 examples/s]\n",
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 122066.72 examples/s]\n",
      "Filter: 100%|██████████| 5921/5921 [00:00<00:00, 123306.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "under_data = undersampling(dataset)\n",
    "# Split the dataset into train and test sets (80% train, 20% test)\n",
    "train_test = under_data.train_test_split(test_size=0.2)\n",
    "\n",
    "# Further split the train set into train and validation sets (75% train, 25% validation of the original 80%)\n",
    "train_valid = train_test['train'].train_test_split(test_size=0.25)\n",
    "\n",
    "# Combine the splits into a DatasetDict\n",
    "under_dataset = DatasetDict({\n",
    "    'train': train_valid['train'],\n",
    "    'validation': train_valid['test'],\n",
    "    'test': train_test['test']\n",
    "})\n",
    "\n",
    "under = under_dataset['test'].to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data = dataset\n",
    "# Split the dataset into train and test sets (80% train, 20% test)\n",
    "train_test = whole_data.train_test_split(test_size=0.2)\n",
    "\n",
    "# Further split the train set into train and validation sets (75% train, 25% validation of the original 80%)\n",
    "train_valid = train_test['train'].train_test_split(test_size=0.25)\n",
    "\n",
    "# Combine the splits into a DatasetDict\n",
    "whole_dataset = DatasetDict({\n",
    "    'train': train_valid['train'],\n",
    "    'validation': train_valid['test'],\n",
    "    'test': train_test['test']\n",
    "})\n",
    "\n",
    "whole = whole_dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(df_subset, df_large=eval):    \n",
    "    # Step 1: Create a dictionary to map each entry to its position in the larger dataset\n",
    "    entry_to_index = {entry: idx for idx, entry in enumerate(df_large['data'])}\n",
    "\n",
    "    # Step 2: Sort the smaller subset by the index positions found in the larger dataset\n",
    "    df_subset['index'] = df_subset['data'].map(entry_to_index)\n",
    "\n",
    "    # Step 3: Drop any rows where the entry wasn't found in the larger dataset\n",
    "    df_subset = df_subset.dropna(subset=['index'])\n",
    "\n",
    "    # Step 4: Convert index column to integer for sorting (optional but useful for correct ordering)\n",
    "    df_subset['index'] = df_subset['index'].astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
